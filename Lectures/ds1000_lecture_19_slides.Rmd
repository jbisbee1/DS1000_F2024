---
title: "Clustering"
subtitle: "Part 2"
author: "Prof. Bisbee"
institute: "Vanderbilt University"
date: "Slides Updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    # self_contained: true
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css:
      - default
      - css/lexis.css
      - css/lexis-fonts.css
    #seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      #ratio: "16:9"

---

```{css,echo = F}
.small .remark-code { /*Change made here*/
  font-size: 85% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
}
```

```{r,include=F}
options(width=60)
knitr::opts_chunk$set(fig.align='center',fig.width=9,fig.height=5)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

# Agenda

1. Tweets as data

2. Words &rarr; topics

3. Application

---

# Social Media

- Unprecedented access to our leaders

--

  - (If they let us)
  
--

<center><img src="https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/themes/2271049/settings_images/TFQe2IEJSWuLQrrYueXd_pasted_image_0_28.png" width="80%"></center>

---

# Social Media

- Unprecedented access to our leaders 

  - (If they let us)

<center><img src="https://media-cldnry.s-nbcnews.com/image/upload/newscms/2020_29/3391138/200618-donald-trump-cellphone-smartphone-cell-ac-1059p.jpg" width="80%"></center>

---

# Social Media

- For researchers, social media is two things

--

  1. A source of .red[data]
  
--

  2. An object of .blue[interest]
  
--

<center><img src="https://ichef.bbci.co.uk/news/1024/cpsprodpb/173D6/production/_116409159_tweet6.png" width="80%"></center>

---

# Twitter as Data

- Not the most popular social media app

<center><img src="https://www.pewresearch.org/internet/wp-content/uploads/sites/9/2021/04/PI_2021.04.07_social-media_0-01.png?w=640" width="60%"></center>

---

# Twitter as Data

- But an outsized platform for the elite

--

- As of 2020

--

  - every U.S. governor had a Twitter account
  
--

  - 49 had a Facebook account
  
--

  - 44 had an Instagram account

--

  - 44 had a YouTube account
  
--

- In professional networks, particularly media, Twitter is almost *lingua franca*

---

# Twitter as Data

- Today?

--

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Twitter has had a massive drop in revenue, due to activist groups pressuring advertisers, even though nothing has changed with content moderation and we did everything we could to appease the activists.<br><br>Extremely messed up! Theyâ€™re trying to destroy free speech in America.</p>&mdash; Elon Musk (@elonmusk) <a href="https://twitter.com/elonmusk/status/1588538640401018880?ref_src=twsrc%5Etfw">November 4, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

---

# Trump and Twitter

- Today, looking at Trump's tweets

--

  - Treating it as a .red[data source]
  
--

  - What can his tweets tell us about the man?
  
```{r,message=F,warning=F}
require(tidyverse)
tweets <- read_rds(file="https://github.com/jbisbee1/DS1000_S2024/raw/main/data/Trumptweets.Rds")
```

--

- The **process**

--

  - Univariate visualisation: how often does he tweet?
  
--

```{r}
p <- tweets %>% 
  count(Tweeting.date) %>%
  ggplot() +
  geom_point(aes(x=Tweeting.date,y=n),alpha=.4) +
  scale_x_date(date_breaks = 'years',date_labels = '%Y') + 
  labs(x="Date",y="Number of Tweets",title="Tweeting by Trump")
```

---

# Trump and Twitter

```{r}
p
```

---

# Trump and Twitter

- Research questions abound!

--

1. What happened in...
  
  - June of 2011?
    
  - June of 2012?
    
  - November of 2016? (duh)
    
--

2. Overtime increase during presidency?

--

3. Others?

---

# Trump and Twitter

- .blue[Research Question:] Did Trump's twitter account benefit from his presidency?

--

```{r,message = F}
require(scales)
p <- tweets %>% 
  group_by(Tweeting.date) %>%
  summarize(AvgRetweet = mean(retweets)) %>%
  ggplot() +
  geom_point(aes(x=Tweeting.date,y=AvgRetweet),alpha=.4) +
  labs(x="Date",y="Average Retweets",title="Tweeting by Trump") +
  scale_y_continuous(label=comma)
```

---

# Trump and Twitter

- .blue[Research Question:] Did Trump's twitter account benefit from his presidency?

- **Yes**

```{r,echo = F}
p
```


---

# Looking with `plotly`

```{r message=FALSE}
library(plotly)
gg <- tweets %>%
  filter(retweets > quantile(retweets,.75)) %>% 
  ggplot(aes(x=Tweeting.date,y=retweets,text=stringr::str_wrap(content,width = 60))) +
  geom_point(alpha=.4) +
  labs(x="Date",y="Retweets",title="Tweeting by Trump") +
  scale_y_continuous(label=scales::comma)
```

---

# Looking with `plotly`

```{r}
ggplotly(gg,tooltip = "text")
```

---

# Trump's Censored Tweets

- What was Trump tweeting about when he was flagged by Twitter?

```{r}
tweets %>%
  filter(is_flagged) %>% # Indicator for Twitter flag
  ggplot(aes(x = Tweeting.date)) + 
  geom_bar()
```

# Trump's Censored Tweets

- Proportion of flagged tweets instead of counts

```{r,echo=F,warning=F,message=F}
gg <- tweets %>%
  count(Tweeting.date,is_flagged) %>%
  group_by(Tweeting.date) %>%
  mutate(totTweets = sum(n)) %>%
  ungroup() %>%
  mutate(prop = n / totTweets) %>%
  filter(is_flagged) %>%
  ggplot(aes(x = Tweeting.date,y = prop)) + 
  geom_point() + 
  scale_y_continuous(labels = scales::percent) + 
  labs(x = 'Date',y = 'Proportion of Total Tweets Flagged')
```

```{r}
gg
```

---

# Looking at content

- We could just look at the flagged tweets themselves

```{r,echo=F}
gg <- tweets %>%
  filter(is_flagged) %>%
  ggplot(aes(x = Tweeting.date,y = retweets,text = stringr::str_wrap(content,width = 60))) + 
  geom_point() + 
  scale_y_continuous(labels = scales::percent) + 
  labs(x = 'Date',y = 'Total Retweets')
```

```{r}
ggplotly(gg)
```

---

# Looking at content

- Or we can look at statistical summaries of Trump's tweets

--

  1. Tweets as .red[data]: we can understand him better
  
  2. Tweets as .blue[object of interest]: how do his tweets influence public discourse?
  
--

- We will be using pre-processed tweets

--

  - Lots of **data wrangling** went into this
  
  - See the `prepping_raw_tweets.R` file for more details

--

```{r}
tweet_words <- read_rds(file="https://github.com/jbisbee1/DS1000_S2024/raw/main/data/Trump_tweet_words.Rds")
```

---

# NLP Definitions

- Before we dig in, some important definitions

--
  
1. **Word / Term:** The core unit of interest

--

  - Often pre-processed to remove "stop words" and to "stem" the words
  
  - "Stop word": an uninteresting, commonly used word
  
  - "Stem / Lemmatize": the core component of a word that contains its meaning (eat, ate, eaten &rarr; eat)
  
--
  
2. **Document:** A collection of words with a single purpose / idea (i.e., a tweet, an essay)

--
  
3. **Corpus:** A collection of documents

--

4. **BOW:** Bag-of-words. Convert a **document** into a count of how many times **words** appear

--

5. **DTM:** Document-term matrix. A dataset where rows are **documents**, columns are **words**, and the values are the counts from **BOW**


---

# What does Trump tweet about?

- Core idea: word frequencies can help:

--

  - Help us understand **documents**
  
--

  - Which help us understand **authors**
  
```{r}
counts <- tweet_words %>% 
  count(word) %>%
  arrange(-n)
```

---

# What does Trump tweet about?

```{r}
counts
```

---

# What does Trump tweet about?

```{r}
p <- tweet_words %>%
  count(word, sort = TRUE) %>% # New ways of doing old things
  head(20) %>%
  ggplot(aes(x = n,y = reorder(word, n))) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  scale_x_continuous(label=comma)
```

---

# What does Trump tweet about?

```{r}
p
```


---

# Effect of becoming president

- Did his focus change when he became president?

```{r}
tweet_words <- tweet_words %>%
  mutate(PostPresident = Tweeting.date > as.Date("2016-11-03"))
```

--

```{r}
p <- tweet_words %>%
  count(PostPresident,word) %>%
  group_by(PostPresident) %>%
  arrange(-n) %>%
  slice(1:10) %>%
  ggplot(aes(x = n,y = reorder(word, n),
             fill = PostPresident)) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  scale_x_continuous(label=comma)
```

---

# Effect of becoming president

```{r}
p
```

---

# Document Term Matrix

- "DTM" counts all the words in each document

--

- In this case, a "document" is a tweet

```{r}
dtm <- tweet_words %>%
  count(document,word)
glimpse(dtm)
```

---

# Document Term Matrix

- However, each tweet is very short

--

- Let's consider the `Tweeting.date` the document

--

  - Concept: What is Trump tweeting about on a given day?
  
```{r}
dtm <- tweet_words %>%
  count(Tweeting.date,word) %>%
  group_by(word) %>%
  mutate(tot_n = sum(n)) %>% # Also calculate TOTAL number of times a word appears
  ungroup()
```

---

# Wrangle

- Extremely rare words should be dropped (typos, etc.)

```{r}
dtm %>%
  arrange(tot_n) %>% head() # Trump tweeted "barnesandnoblecom" only once in his life

dtm <- dtm %>%
  filter(tot_n > 20) # Drop these rarely occurring words
```


---

# Analyzing BOW

- Some words are frequently found in many documents

--

- We want to find words that are **uniquely** used

--

  - "Unique" &rarr; used frequently in one document but not in any others
  
--

- "TF-IDF": Term frequency-inverse document frequency

--

- "TF": $\frac{\text{word count}}{\text{total words}}$

- "DF": $\frac{\text{documents with word}}{\text{total documents}}$

--

  - "IDF": Just invert it $\frac{\text{total documents}}{\text{documents with word}}$
  
$$tf-idf(w,d) = tf(w,d) \times log \left( \frac{N}{df(w)}\right) $$

---

# TF-IDF

```{r,warning=F,message=F}
require(tidytext) # Required to calculate TF-IDF
dtm.tfidf <- bind_tf_idf(tbl = dtm, term = word, document = Tweeting.date, n = n) # Calculate TF-IDF
dtm.tfidf  %>%
  select(word,tf_idf) %>%
  distinct() %>%
  arrange(-tf_idf) %>%
  slice(1:10)
```

---

# $K$-means

- How to summarize this? $k$-means clustering!

--

- Recall that `kmeans()` function clusters over every column in a data frame

--

- `dtm.tfidf` is organized "long" (i.e., each row is a word-by-document)

--

- Want to convert to "wide" (i.e., rows are documents, columns are words)

--

- Previously used `spread()`, but for *k*-means with text: `cast_dtm()`

--

```{r}
castdtm <- cast_dtm(data = dtm.tfidf, document = Tweeting.date, term = word, value = tf_idf)
```

--

- Now let's calculate `kmeans()` (this will take a few seconds)

```{r}
set.seed(42)
km_out <- kmeans(castdtm, 
                 centers = 50, # Number of "topics"
                 nstart = 5)
```

---

# Looking at Clusters

- Some quick wrangling

```{r,warning=F,message=F}
require(tidymodels)
km_out_tidy <- tidy(km_out) %>%
  gather(word,mean_tfidf,-size,-cluster,-withinss) %>% # Convert to long data
  mutate(mean_tfidf = as.numeric(mean_tfidf)) # Calculate average TF-IDF
km_out_tidy
```

---

# Looking at Clusters

- And can plot! (Just look at first 10 "topics")

```{r}
p <- km_out_tidy %>%
  filter(cluster %in% 1:9) %>%
  group_by(cluster) %>%
  arrange(-mean_tfidf) %>%
  slice(1:10) %>%
  ggplot(aes(x = mean_tfidf,y = reorder(word,mean_tfidf),
             fill = factor(cluster))) + 
  geom_bar(stat = 'identity') + 
  facet_wrap(~cluster,scales = 'free') + 
  labs(title = 'k-means Clusters',
       subtitle = 'Clustered by TF-IDF',
       x = 'Centroid',
       y = NULL,
       fill = 'Cluster ID')
```

---

# Looking at Clusters

```{r}
p
```

---

# Looking at clusters

- What are these clusters?

--

- Let's look at the most popular topics

```{r}
(tops <- km_out_tidy %>%
  select(size,withinss,cluster) %>%
  distinct() %>%
  arrange(desc(size)) %>%
    slice(1:5))

p <- km_out_tidy %>%
  filter(cluster %in% tops$cluster) %>%
  group_by(cluster) %>%
  arrange(-mean_tfidf) %>%
  slice(1:10) %>%
  ggplot(aes(x = mean_tfidf,y = reorder(word,mean_tfidf),
             fill = factor(cluster))) + 
  geom_bar(stat = 'identity') + 
  facet_wrap(~cluster,scales = 'free') + 
  labs(title = 'k-means Clusters',
       subtitle = 'Clustered by TF-IDF',
       x = 'Centroid',
       y = NULL,
       fill = 'Cluster ID')
```

---

# Looking at clusters

```{r}
p
```

---

# Optimal number of clusters?

- We know how to do this already! Elbow plot!

```{r}
set.seed(42)
totWSS <- NULL
for(k in c(1,10,50,100,250,500,1000)) {
  km_out <- kmeans(castdtm, 
                 centers = k,
                 nstart = 5)
  
  totWSS <- data.frame(totWSS = km_out$tot.withinss,
                       k = k) %>%
    bind_rows(totWSS)
}

p <- totWSS %>%
  ggplot(aes(x = k,y = totWSS)) + 
  geom_point() + 
  geom_line()
```

---

# Optimal number of clusters?

```{r}
p
```



---

# Conclusion

- $k$-means clustering on text &rarr; **topics**

--

- As always, this is a deep area of study

--

  - Superior methods are out there (Latent Dirichlet Allocation, Structural Topic Modeling, etc.)
  
--

- NOTE: even with text, always start with simple descriptives

--

  - **Looking** at your data is the heart of data science


