---
title: "Text, Tweets, and Sentiment"
subtitle: "Part 3"
author: "Prof. Bisbee"
institute: "Vanderbilt University"
date: "Slides Updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    # self_contained: true
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css:
      - default
      - css/lexis.css
      - css/lexis-fonts.css
    #seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      #ratio: "16:9"

---

```{r,include=F}
options(width=60)
knitr::opts_chunk$set(fig.align='center',fig.width=9,fig.height=5)
```

---

# Returning to Trump

```{r,message=F,warning=F}
require(tidyverse)
tweet_words <- read_rds(file="https://github.com/jbisbee1/DS1000_S2024/raw/main/data/Trump_tweet_words.Rds")
tweet_words <- tweet_words %>% mutate(PostPresident = Tweeting.date > as.Date('2016-11-06'))
```

---

# Log-Odds

- **Odds**: Probability a word is used pre/post presidency
  
- **Log**: Useful for removing skew in data!
  
--

- Interactive code time!

---

# Odds Step 1

```{r}
(odds1 <- tweet_words %>%
  count(word, PostPresident) %>%
  filter(sum(n) >= 5) %>%
  spread(PostPresident, n, fill = 0) %>%
  ungroup() %>%
  mutate(totFALSE = sum(`FALSE`),
         totTRUE = sum(`TRUE`)))
```

---

# Odds Step 2

```{r}
(odds2 <- odds1 %>%
  mutate(propFALSE = (`FALSE` + 1) / (totFALSE + 1),
         propTRUE = (`TRUE` + 1) / (totTRUE + 1)))
```

---

# Odds Step 3

```{r}
(odds3 <- odds2 %>%
  mutate(odds = propTRUE / propFALSE))
```

---

# Why log?

```{r,message=F}
odds3 %>%
  ggplot(aes(x = odds)) + 
  geom_histogram()
```

---

# Why log?

```{r}
odds3 %>%
  ggplot(aes(x = odds)) + 
  geom_histogram(bins = 15) + 
  scale_x_log10()
```

---

# Odds Step 4

```{r}
(prepost_logodds <- odds3 %>%
  mutate(logodds = log(odds)))
```

---

# Effect of becoming president

```{r, warning=FALSE}
p <- prepost_logodds %>%
  group_by(logodds > 0) %>%
  top_n(15, abs(logodds)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logodds)) %>%
  ggplot(aes(word, logodds, fill = logodds < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Post-President/Pre-President log ratio") +
  scale_fill_manual(name = "", labels = c("President", "Pre-President"),
                    values = c("red", "lightblue"))
```

---

# Effect of becoming president

```{r}
p
```

---

# Meaning

- Thus far, everything is **topic**-related

--

  - How often he talks about things
  
--

- But what does he **mean** when he talks about Mueller?

--

  - We can probably guess
  
--

- But we want a more systematic method

--

  - **Sentiment**: the *feeling* behind words
  
---

# Meaning

- **Sentiment** analysis is based on **dictionaries**

--

  - Just like **stop words** from last week!
  
--

  - Prepared lists of words, but tagged according to **emotion**
  
--

- Good dictionary included in `tidytext` package

```{r,warning = F}
require(tidytext) # Might need to install.packages('textdata')
# nrc <- get_sentiments("nrc")
# If this doesn't work on your computer, just load it with read_rds()
nrc <- read_rds('https://github.com/jbisbee1/DS1000_S2024/raw/main/data/nrc.Rds')
```

---

# Meaning

```{r}
nrc
```

---

# Sentiment by Pre/Post Presidency

- Measure sentiment by proportion of words

--

- Divide by pre/post presidency

--

```{r}
word_freq <- tweet_words %>%
  group_by(PostPresident) %>%
  count(word) %>%
  filter(sum(n) >= 5) %>%
   mutate(prop = prop.table(n)) # Faster way of calculating proportions!
```

---

# Sentiment by Pre/Post Presidency

- Attaching sentiment from `nrc`

--

  - `inner_join()`: only keeps words that appear in `nrc`
  
```{r}
word_freq_sentiment <- word_freq %>%
    inner_join(nrc, by = "word") 
```

---

# Sentiment overall

```{r}
p <- word_freq_sentiment %>%
  group_by(sentiment) %>%
  top_n(10, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(y = word, x = n)) +
  facet_wrap(~ sentiment, scales = "free", nrow = 3) + 
  geom_bar(stat = "identity")
```

---

# Sentiment Overall

```{r}
p
```

---

# Sentiment overall

- Could also just calculate positive sentiments - negative sentiments

--

  - Want to do this at the tweet level
  
--

```{r,warning=F,message=F}
tweet_sentiment <- tweet_words %>%
    inner_join(nrc, by = "word") 
  
tweet_sentiment_summary <- tweet_sentiment %>%
  group_by(PostPresident, sentiment) %>%
  count(document,sentiment) %>%
  pivot_wider(names_from = sentiment, 
              values_from = n, 
              values_fill = 0) %>% # same as spread()!
  mutate(sentiment = positive - negative)
```

---

# Sentiment overall

```{r}
tweet_sentiment_summary
```

---

# Sentiment by presidency

- Calculate total number of tweets by sentiment

--

```{r}
tweet_sentiment_summary  %>%
  group_by(PostPresident) %>%
  mutate(ntweet = 1) %>%
  summarize(across(-document, sum)) 
```

---

# Sentiment by presidency

- Univariate distributions!

--

```{r,warning = FALSE}
p <- tweet_sentiment_summary %>%
  ggplot(aes(x = sentiment, y = PostPresident)) + 
  geom_boxplot() +
  labs(y= "Trump is president", x = "Sentiment Score: Positive - Negative")
```



---

# Sentiment by presidency

- Univariate distributions!

```{r,message=F,warning = FALSE}
p
```

---

# Sentiment by hour

- Univariate distributions

  - Comparing sentiment by hour
  
--

```{r,message=F}
p <- tweet_sentiment %>%
  group_by(PostPresident,Tweeting.hour,sentiment) %>%
  count(document,sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>%
  summarize(AvgSentiment = mean(sentiment)) %>%
  ggplot(aes(y = AvgSentiment, x= Tweeting.hour, color=PostPresident)) + 
  geom_point(size = 4) +
  geom_hline(yintercept = 0,linetype = 'dashed') + 
  labs(x = "Tweeting Hour (EST)", y = "Average Tweet Sentiment: Positive - Negative", color = "Is President?")
```

---

# Sentiment by hour

- Comparing sentiment by hour

```{r}
p
```


---

# Understanding Trump

- When Trump is coded as "positive" or "negative", what is he saying?

--

- Look at log-odds ratio words, matched to sentiment!

```{r}
p <- prepost_logodds %>%
  inner_join(nrc, by = "word") %>%
  filter(!sentiment %in% c("positive", "negative")) %>%
  mutate(sentiment = reorder(sentiment, -logodds),
         word = reorder(word, -logodds)) %>%
  group_by(sentiment) %>%
  top_n(10, abs(logodds)) %>%
  ungroup() %>%
  ggplot(aes(y = word, x = logodds, fill = logodds < 0)) +
  facet_wrap(~ sentiment, scales = "free", nrow = 2) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "", y = "Post / Pre log ratio") +
  scale_fill_manual(name = "", labels = c("Post", "Pre"),
                    values = c("red", "lightblue")) + 
  theme(legend.position = 'bottom')
```

---

# Understanding Trump

```{r}
p
```

---

# Text as predictors

- Let's say we didn't know when each tweet was written

--

- Could we predict whether it was written during his presidency or not?

--

  - Logit model using **text** as predictors
  
  
---

# Text as Data

- Predict tweets by average of words' log-odds!

```{r,message=F}
toanal <- tweet_words %>%
  select(document,word,PostPresident) %>%
  left_join(prepost_logodds %>% select(word,logodds)) %>% # Link data with log-odds
  group_by(document,PostPresident) %>%
  summarise(logodds = mean(logodds)) %>% # Calculate average log-odds by document
  ungroup()

m <- glm(PostPresident ~ logodds,toanal,family = binomial) # logit regression
```

---

# Text as Data

- Evaluate the performance

```{r,message=F,warning=F}
require(tidymodels)
forAUC <- toanal %>% # Evaluate model performance
  mutate(preds = predict(m,type = 'response'),
         truth = factor(PostPresident,levels = c('TRUE','FALSE')))

roc_auc(forAUC,'truth','preds')

p <- roc_curve(forAUC,'truth','preds') %>%
  ggplot(aes(x = 1-specificity,y = sensitivity)) + 
  geom_line() + 
  geom_abline(intercept = 0,slope = 1,linetype = 'dashed')
```

---

# Evaluate performance

```{r}
p
```

---

# Evaluate on some sample tweets

```{r}
raw_tweets <- read_rds('../data/Trumptweets.Rds')
set.seed(20)
toCheck <- raw_tweets %>% slice(sample(1:nrow(.),size = 10))

toCheck %>%
  select(content)
```

---

# Evaluate on some sample tweets

```{r}
toTest <- toCheck %>% left_join(toanal,by = c('id' = 'document')) # Merge the raw text with the log-odds

toTest %>%
  mutate(preds = predict(m,newdata = toTest,type = 'response')) %>%
  select(content,PostPresident,preds) %>%
  mutate(pred_binary = preds > .5) %>%
  filter(PostPresident != pred_binary)
# We only make 3 mistakes!
```


---

# Can we do better if we add sentiment?

```{r}
toanal <- toanal %>%
  left_join(tweet_sentiment_summary) %>%
  drop_na()

m1 <- glm(PostPresident ~ logodds,toanal,family = binomial)
m2 <- glm(PostPresident ~ logodds + sentiment,toanal,family = binomial)
m3 <- glm(PostPresident ~ logodds + anger + anticipation + disgust + fear + joy + sadness + surprise + trust,toanal,family = binomial)

forAUC <- toanal %>%
  mutate(preds1 = predict(m1,type = 'response'),
         preds2 = predict(m2,type = 'response'),
         preds3 = predict(m3,type = 'response'),
         truth = factor(PostPresident,levels = c('TRUE','FALSE')))
```

---

# Can we do better if we add sentiment?

```{r}
roc_auc(forAUC,'truth','preds1') %>% mutate(model = 'logodds') %>%
  bind_rows(roc_auc(forAUC,'truth','preds2') %>% mutate(model = 'logodds & net sentiment')) %>%
  bind_rows(roc_auc(forAUC,'truth','preds3') %>% mutate(model = 'logodds & detailed sentiment'))
```

--

- Not really

---

# Conclusion

- Sentiment can...

--

  - ...help us describe the data (i.e., infer what someone meant)
  
  - ...help us predict the data (RQ: do positive tweets get more likes?)
