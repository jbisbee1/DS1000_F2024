---
title: "Problem Set 7"
subtitle: "Multivariate Visualization"
author: "[YOUR NAME]"
institute: "Vanderbilt University"
date: "Due Date: 2024-10-25"
output:
  html_document: default
---

```{r,include=F}
knitr::opts_chunk$set(error=TRUE)
```


## Getting Set Up

Open `RStudio` and create a new RMarkDown file (`.Rmd`) by going to `File -> New File -> R Markdown...`.
Accept defaults and save this file as `[LAST NAME]_ps7.Rmd` to your `code` folder.

Copy and paste the contents of this `.Rmd` file into your `[LAST NAME]_ps7.Rmd` file. Then change the `author: [Your Name]` to your name.

We will be using a new dataset called `youtube_individual.rds` which can be found on the course [github page](https://github.com/jbisbee1/ISP_Data_Science_2024/raw/main/data/youtube_individual.rds). The codebook for this dataset is produced below. All ideology measures are coded such that negative values indicate more liberal content and positive values indicate more conservative content.

| Name           | Description                                                                               |
|----------------|:------------------------------------------------------------------------------------------|
| ResponseId         | A unique code for each respondent to the survey   |
| ideo_recommendation   | The average ideology of all recommendations shown to the respondent             |
| ideo_current   | The average ideology of all current videos the respondent was watching when they were shown recommendations      |
| ideo_watch        | The average ideology of all videos the respondent has ever watched on YouTube (their "watch history") |
| nReccs       | The total number of recommendations the respondent was shown during the survey                                    |
| YOB           | The year the respondent was born                                              |
| education     | The respondent's highest level of education                                  |
| gender   | The respondent's gender                               |
| income   | The respondent's total household income                               |
| party_id   | The respondent's self-reported partisanship                               |
| ideology   | The respondent's self-reported ideology                               |
| race   | The respondent's race                               |
| age   | The respondent's age at the time of the survey                               |

All of the following questions should be answered in this `.Rmd` file. There are code chunks with incomplete code that need to be filled in. 

This problem set is worth 5 total points, plus **one** extra credit question. However, there are also additional opportunities for additional points in problem 5 and the extra credit question itself. Note that these additional points are **very hard**. They are designed for the students who claimed that the course is easy on the midterm evals. I encourage you all to attempt all extra credit, but don't worry if you don't get the super hard ones. 

The point values for each question are indicated in brackets below. To receive full credit, you must have the correct code. In addition, some questions ask you to provide a written response in addition to the code.
All of the following questions should be answered in this `.Rmd` file. There are code chunks with incomplete code that need to be filled in. To submit, compile (i.e., `knit`) the completed problem set and upload the PDF file to Brightspace on Friday by midnight. Instructions for how to compile the output as a PDF can be found in [Problem Set 0](https://github.com/jbisbee1/DS1000_F2024/blob/main/Psets/ds1000_pset_0.pdf) and in this [gif tutorial](https://github.com/jbisbee1/DS1000_F2024/blob/main/Psets/save_as_pdf.gif). 

This problem set is worth 5 total points, plus 1 extra credit point. The point values for each question are indicated in brackets below. To receive full credit, you must have the correct code. In addition, some questions ask you to provide a written response in addition to the code.

You will be deducted 1 point for each day late the problem set is submitted, and 1 point for failing to submit in the correct format.

You are free to rely on whatever resources you need to complete this problem set, including lecture notes, lecture presentations, Google, your classmates...you name it. However, the final submission must be complete by you. There are no group assignments.

*Note that the TAs and professors will not respond to Campuswire posts after 5PM on Friday, so don't wait until the last minute to get started!*

**Good luck!**

*Copy the link to ChatGPT you used here: _________________

## Question 0
*Require `tidyverse` and load the `youtube_individual.rds` data to an object called `yt`.*

```{r}
require(tidyverse)
yt <- read_rds("https://github.com/jbisbee1/DS1000_F2024/raw/main/data/youtube_individual.rds")
```

## Question 1 [1 point]
*We are interested in how the YouTube recommendation algorithm works. These data are collected from real users, logged into their real YouTube accounts, allowing us to see who gets recommended which videos. We will investigate three research questions in this problem set:*

*1. What is the relationship between average ideology of recommendations shown to each user, and the average ideology of all the videos the user has watched?*

*2. What is the relationship between the average ideology of recommendations shown to each user, and the average ideology of the current video the user was watching when they were shown the recommendation?*

*3. Which of these relationships is stronger? Why?*

*Start by answering all three of these research questions, and explaining your thinking. Be very precise about your assumptions!*

> I assume that the recommendation algorithm is designed to suggest videos that users will likely watch. I assume that the algorithm learns about what the user likes to watch on the basis of what they are currently watching, and on the basis of what they historically have chosen to watch. Thus for both questions 1 and 2, I hypothesize that the relationship between the average ideology of recommendations and the average ideology of either the current video or the user's watch history are both positive. This means that users who are currently watching conservative videos will be recommended more conservative videos, and that users who historically watch conservative videos will be recommended more conservative videos. [RUBRIC: 0.75 points total - answers should explain what they assume about how the recommendation algorithm works.] 

> However, I think that the user's watch history contains more information about their preferences, and therefore should be the stronger relationship. [RUBRIC: 0.25 points - answers should explain why they think one or the other is a stronger predictor.]

## Question 2 [1 point]

*Based on your previous answer, which variables are the $X$ (predictors) and which are the $Y$ (outcome) variables?*

> The predictors are the current video ideology and the watch history ideology. The outcome variable is the recommendation ideology. [RUBRIC: 0.25 points - either right or wrong...no partial credit here.]

*Now create univariate visualizations of all three variables, making sure to label your plots clearly.*

```{r}
# Y: average_recommendation_ideo [RUBRIC: 0.25 points - deduct 0.1 points for the wrong geom_...() and 0.1 points for bad labels]
yt %>%
  ggplot(aes(x = ideo_recommendation)) + # Put the outcome variable on the x-axis
  geom_histogram() +  # Choose the best geom_...() to visualize based on the variable's type
  labs(x = 'Recommendation Ideology', # Provide clear labels to help a stranger understand!
       y = 'Number of users',
       title = 'Univariate visualization of recommendation ideology')

# X2: average_watch_ideo [RUBRIC: 0.25 points - deduct 0.1 points for the wrong geom_...() and 0.1 points for bad labels]
yt %>%
  ggplot(aes(x = ideo_watch)) + # Put the first explanatory variable on the x-axis
  geom_histogram() +  # Choose the best geom_...() to visualize based on the variable's type
  labs(x = 'Watch History Ideology', # Provide clear labels to help a stranger understand!
       y = 'Number of users',
       title = 'Univariate visualization of watch history ideology')

# X2: average_current_ideo [RUBRIC: 0.25 points - deduct 0.1 points for the wrong geom_...() and 0.1 points for bad labels]
yt %>%
  ggplot(aes(x = ideo_current)) + # Put the second explanatory variable on the x-axis
  geom_histogram() +  # Choose the best geom_...() to visualize based on the variable's type
  labs(x = 'Current Video Ideology', # Provide clear labels to help a stranger understand!
       y = 'Number of users',
       title = 'Univariate visualization of current video ideology')
```

## Question 3 [1 point]

*Let's focus on the first research question. Create a multivariate visualization of the relationship between these two variables, making sure to put the $X$ variable on the x-axis, and the $Y$ variable on the y-axis. Add a straight line of best fit. Does the data support your theory?*

```{r}
yt %>%
  ggplot(aes(x = ideo_watch,  # Put the first explanatory variable on the x-axis
             y = ideo_recommendation)) +  # Put the outcome variable on the y-axis
  geom_point() + # Choose the best geom_...() for visualizing this type of multivariate relationship
  geom_smooth(method = 'lm') +  # Add a straight line of best fit
  labs(x = 'Watch history ideology', # Provide clear labels to help a stranger understand!
       y = 'Recommendation ideology',
       title = 'Relationship between watch history and recommendation ideology')
```

> Yes the data strongly supports my theory. There is clear evidence of a positive relationship between a user's watch history ideology and the recommendation ideology they are being shown.[RUBRIC: 0.25 points - deduct 0.1 points for the wrong geom_...() and 0.1 points for bad labels. Deduct 0.15 points for poorly written answer.]

*Now run a linear regression using the `lm()` function and save the result to an object called `model_watch`.*

```{r}
# [RUBRIC: 0.25 points - either right or wrong.]
model_watch <- lm(formula = ideo_recommendation ~ ideo_watch, # Write the regression equation here (remember to use the tilde ~!)
             data = yt) # Indicate where the data is stored here.
```

*Using either the `summary()` function (from base `R`) or the `tidy()` function (from the `broom` package), print the regression result.*

```{r}
require(broom)
tidy(model_watch)
```

*In a few sentences, summarize the results of the regression output. This requires you to translate the statistical measures into plain English, making sure to refer to the units for both the $X$ and $Y$ variables. In addition, you must determine whether the regression result supports your hypothesis, and discuss your confidence in your answer, referring to the p-value.*

> According to this model, the average ideology of a user's recommendations is 0.08 when their watch history ideology is zero, meaning a totally moderate watch history. The model also tells us that the average ideology of recommendations shown to users increases by 0.558 units when their watch history ideology increases by 1 unit. I am more than 99.99% confident that this positive relationship is not zero, meaning it is a strong relationship. [RUBRIC: 0.5 points - deduct 0.1 points for not interpreting the intercept value. Deduct 0.2 points for incorrectly describing the meaning of the slope. Deduct 0.1 points for not expressing their confidence correctly.]

## Question 4 [1 point]

*Now let's do the same thing for the second research question. First, create the multivariate visualization and determine whether it is consistent with your theory.*

```{r}
yt %>%
  ggplot(aes(x = ideo_current,  # Put the second explanatory variable on the x-axis
             y = ideo_recommendation)) +  # Put the outcome variable on the y-axis
  geom_point() + # Choose the best geom_...() for visualizing this type of multivariate relationship
  geom_smooth(method = 'lm') +  # Add a straight line of best fit
  labs(x = 'Current video ideology', # Provide clear labels to help a stranger understand!
       y = 'Recommendation ideology',
       title = 'Relationship between current video and recommendation ideology')
```

> This relationship is even stronger, suggesting a very strong positive relationship between the ideology of the video the user is currently watching and the videos they are being recommended by YouTube's algorithm. [RUBRIC: 0.25 points - deduct 0.1 points for the wrong geom_...() and 0.1 points for bad labels]

*Second, run a new regression and save the result to `model_current`. Then print the result using either `summary()` or `tidy()`, as before.*

```{r}
# [RUBRIC: 0.25 points - either right or wrong.]
model_current <- lm(formula = ideo_recommendation ~ ideo_current, # Write the regression equation here (remember to use the tilde ~!)
             data = yt) # Indicate where the data is stored here.

tidy(model_current)
```

*Finally, describe the result in plain English, and interpret it in light of your hypothesis. How confident are you?*

> According to this model, the average ideology of a user's recommendations is 0.058 when the current video ideology is zero, meaning the recommendations that are shown on a totally moderate video are a tiny bit conservative. The model also tells us that the average ideology of recommendations shown to users increases by 0.743 units when the ideology of the current video increases by 1 unit. I am more than 99.99% confident that this positive relationship is not zero, meaning it is a strong relationship. [RUBRIC: 0.25 points - deduct 0.05 points for not interpreting the intercept value. Deduct 0.1 points for incorrectly describing the meaning of the slope. Deduct 0.05 points for not expressing their confidence correctly.]

*Based* **ONLY** *on the preceding analysis, are you able to answer research question 3?*

> While both watch history and current video ideology are strong predictors of the ideology of recommendations, the relationship between the current video and recommendations is stronger than the relationship between watch history and recommendations. The beta coefficient is 0.743, compared to 0.558, and the p-value is effectively zero, compared to 3.31e-57. While model variables are very strong predictors, it appears that the current video ideology is a stronger predictor of recommendation ideology. [RUBRIC: 0.25 points - be generous here. As long as they say something about EITHER coefficient magnitude OR p-value, give full credit.]

## Question 5 [1 point + 2 EC points]
*Now let's evaluate the models. Start by calculating the "mistakes" (i.e., the "errors" or the "residuals") generated by both models and saving these as new columns (`errors_watch` and `errors_current`) in the `yt` dataset.*

```{r}
# Calculating errors [RUBRIC: 0.1 point - either they get it or they don't]
yt <- yt %>%
  mutate(preds_watch = predict(model_watch), # Get the predicted values from the first model (Yhat)
         preds_current = predict(model_current)) %>% # Get the predicted values from the second model (Yhat)
  mutate(errors_watch = ideo_recommendation - preds_watch, # Calculate errors for the first model (Y - Yhat)
         errors_current = ideo_recommendation - preds_current) # Calculate errors for the second model (Y - Yhat)
```

*Now create two univariate visualization of these errors. Based on this result, which model looks better? Why? EC [+1 point]: Plot both errors on the same graph using `pivot_longer()`.*

```{r}
# Univariate visualization of watch history model errors [RUBRIC: 0.1 points - deduct 0.05 points for the wrong geom_...() and 0.05 points for bad labels]
yt %>%
  ggplot(aes(x = errors_watch)) + # Put the errors from the first model on the x-axis
  geom_histogram() +  # Choose the best geom_...() to visualize based on the variable's type
  labs(x = 'Model 1 errors', # Provide clear labels to help a stranger understand!
       y = 'Number of users',
       title = 'Univariate visualization of model 1 errors',
       subtitle = 'Recommendation Ideology ~ Watch History Ideology')

# Univariate visualization of current video model errors [RUBRIC: 0.1 points - deduct 0.05 points for the wrong geom_...() and 0.05 points for bad labels]
yt %>%
  ggplot(aes(x = errors_current)) + # Put the errors from the second model on the x-axis
  geom_histogram() +  # Choose the best geom_...() to visualize based on the variable's type
  labs(x = 'Model 2 errors', # Provide clear labels to help a stranger understand!
       y = 'Number of users',
       title = 'Univariate visualization of model 2 errors',
       subtitle = 'Recommendation Ideology ~ Current Video Ideology')

# EC [1 point]: Plot both errors on a single plot. Hint: use pivot_longer().
yt %>%
  select(ResponseId,matches('errors')) %>%
  pivot_longer(names_to = 'model',
               values_to = 'errors',
               cols = -ResponseId) %>%
  ggplot(aes(x = errors,
             fill = model)) + 
  geom_density(alpha = .3) + 
  labs(x = 'Model errors',
       y = 'Density',
       fill = 'Model',
       title = 'Comparing Model 1 and 2 errors')
```

> Both models appear to do fairly well. However, the current video model's errors are more tightly clustered around zero, and are also more symmetrically distributed. As such, I would argue that model 2 is the better fit. [RUBRIC: 0.25 points - deduct 0.1 point for not emphasizing the variance of the distribution and 0.1 point for not discussing symmetricality]

*Finally, create a multivariate visualization of both sets of errors, comparing them against the $X$ variable. Based on this result, which model looks better? Why? EC [+1 point]: Create two plots side-by-side using facet_wrap(). This is SUPER HARD, so don't worry if you can't get it.*

```{r}
# Multivariate visualization of watch history errors [RUBRIC: 0.1 points - deduct 0.05 points for the wrong geom_...() and 0.05 points for bad labels]
yt %>%
  ggplot(aes(x = ideo_watch,      # Put the predictor on the x-axis
             y = errors_watch)) + # Put the errors on the y-axis
  geom_point() + # Choose the best geom_...()
  geom_smooth() + # Add a curvey line of best fit
  geom_hline(yintercept = 0,linetype = 'dashed') + # Add a horizontal dashed line at zero
  labs(x = 'Average watch ideology', # Give it clear labels
       y = 'Errors: Watch ideology model',
       title = 'Relationship between errors and predictor',
       subtitle = 'Average Watch Ideology')

# Multivariate visualization of current video errors [RUBRIC: 0.1 points - deduct 0.05 points for the wrong geom_...() and 0.05 points for bad labels]
yt %>%
  ggplot(aes(x = ideo_current,      # Put the predictor on the x-axis
             y = errors_current)) + # Put the errors on the y-axis
  geom_point() + # Choose the best geom_...()
  geom_smooth() + # Add a curvey line of best fit
  geom_hline(yintercept = 0,linetype = 'dashed') + # Add a horizontal dashed line at zero
  labs(x = 'Average current ideology', # Give it clear labels
       y = 'Errors: current ideology model',
       title = 'Relationship between errors and predictor',
       subtitle = 'Average Current Video Ideology')

# EC [1 point]: Try to create two plots side-by-side. (SUPER HARD)
yt %>%
  select(ideo_watch,
         ideo_current,
         errors_watch,
         errors_current,
         ResponseId) %>%
  pivot_longer(cols = -ResponseId,
               names_to = c('variable','model'),
               names_pattern = '(.*)_(.*)') %>%
  pivot_wider(names_from = variable,
              values_from = value) %>%
  ggplot(aes(x = ideo,
             y = errors)) + 
  geom_point() + 
  geom_smooth() + 
  geom_hline(yintercept = 0,linetype = 'dashed') + 
  facet_wrap(~model) + 
  labs(x = 'Ideology of X Variable',
       y = 'Prediction Errors',
       title = 'Multivariate visualization of errors',
       subtitle = 'Comparing model 1 and 2 side-by-side')
```

> The multivariate visualization of the errors indicates that the current video model is a better fit than the watch history model since the points are more tightly clustered around zero and the line is flatter. [RUBRIC: 0.25 points - deduct 0.1 point for not emphasizing the variance of the distribution and 0.1 point for not discussing the flatness of the line]

## Extra Credit [1 point]
*Calculate the* **R***oot* **M***ean* **S***quared* **E***rror (RMSE) using 100-fold cross validation with a 50-50 split for both models. How bad are the first model's mistakes on average? How bad are the second model's mistakes? Which model seems better? Remember to talk about the result in terms of the range of values of the outcome variable! EC [+1 point]: plot the errors by the model using geom_boxplot(). HINT: you'll need to use pivot_longer() to get the data shaped correctly.*

```{r,message=F,warning=F}
set.seed(123) # Set the seed to ensure replicability
cvRes <- NULL # Instantiate an empty object to save the results
for(i in 1:100) { # 100-fold cross validation
  # Create the training dataset
  train <- yt %>%
    sample_n(size = round(nrow(.)*.5), # set the size equal to half of the original dataset
             replace = F) # Make sure to NOT replace observations (unlike bootstrapping!)
  
  # Create the testing dataset
  test <- yt %>%
    anti_join(train) # Use anti_join() to make the test set contain every observation NOT in the train set
  
  # Estimate model 1 on the training dataset
  mTmp_watch <- lm(ideo_recommendation ~ ideo_watch,
             data = train)
  
  # Estimate model 2 on the training dataset
  mTmp_current <- lm(ideo_recommendation ~ ideo_current,
             data = train)
  
  # Predict both models on the testing dataset
  test <- test %>%
    mutate(preds_watch = predict(mTmp_watch,newdata = test),
           preds_current = predict(mTmp_current,newdata = test))
  
  # Calculate the RMSE
  answer <- test %>%
    mutate(errors_watch = ideo_recommendation - preds_watch, # calculate the errors for model 1
           errors_current = ideo_recommendation - preds_current) %>% # calculate the errors for model 2
    mutate(se_watch = errors_watch^2, # Square the errors
           se_current = errors_current^2) %>%
    summarise(mse_watch = mean(se_watch,na.rm=T), # Take the mean of the square errors
              mse_current = mean(se_current,na.rm=T)) %>%
    mutate(rmse_watch = sqrt(mse_watch), # Take the square root of the mean of the square errors
           rmse_current = sqrt(mse_current),) %>%
    mutate(cvInd = i) # Add the cross validation index
  
  # Save the result
  cvRes <- cvRes %>%
    bind_rows(answer)
}

# Finally, calculate the RMSE value
mean(cvRes$rmse_watch)
mean(cvRes$rmse_current)

# EC [1 point]: 
cvRes %>%
  select(rmse_watch,rmse_current,cvInd) %>%
  pivot_longer(cols = c(rmse_watch,rmse_current),
               names_to = 'model',
               values_to = 'rmse') %>%
  ggplot(aes(x = rmse,
             y = model)) + 
  geom_boxplot() + 
  labs(x = '100-fold CV RMSE',
       y = 'Model',
       title = 'RMSE by model')
```

> The comparison of RMSE clearly illustrates the superior fit of the current YouTube video compared to the watch history. The watch history model has an RMSE of roughly 0.14, while the current video model has an RMSE of roughly 0.064. Both of these are relatively small RMSE values when compared to the outcome variable, which ranges from -0.5 to +0.5. But the current video model is clearly superior. [RUBRIC: 1 point - deduct 0.25 points for not calculating both RMSEs in the same loop. Deduct 0.25 points for not calculating the RMSE correctly (although be flexible on how they do it). Deduct 0.25 points for not referring to the distribution of the outcome variable when discussing the scale of the RMSE results.]

