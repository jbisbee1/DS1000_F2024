---
title: "Problem Set 9"
subtitle: "Clustering"
author: "[YOUR NAME]"
institute: "Vanderbilt University"
date: "Due Date: 2024-12-06"
output:
  html_document: default
---

```{r,include=F}
knitr::opts_chunk$set(error=TRUE)
```

## Getting Set Up

Open `RStudio` and create a new RMarkDown file (`.Rmd`) by going to `File -> New File -> R Markdown...`.
Accept defaults and save this file as `[YOUR NAME]_ps9.Rmd` to your `code` folder.

Copy and paste the contents of this `.Rmd` file into your `[YOUR NAME]_ps9.Rmd` file. Then change the `author: [Your Name]` on line 2 to your name.

We will be using two datasets. The first is the `pres_elec.rds` file from the course [github page](https://github.com/jbisbee1/DS1000_F2024/raw/main/data/pres_elec.rds). The second is the `Trump_tweet_words.Rds` file from the course [github page](https://github.com/jbisbee1/DS1000_F2024/raw/main/data/Trump_tweet_words.Rds).

All of the following questions should be answered in this `.Rmd` file. There are code chunks with incomplete code that need to be filled in. 

The point values for each question are indicated in brackets below. To receive full credit, you must have the correct code. In addition, some questions ask you to provide a written response in addition to the code.
All of the following questions should be answered in this `.Rmd` file. There are code chunks with incomplete code that need to be filled in. To submit, compile (i.e., `knit`) the completed problem set and upload the PDF file to Brightspace on Friday by midnight. Instructions for how to compile the output as a PDF can be found in [Problem Set 0](https://github.com/jbisbee1/DS1000_F2024/blob/main/Psets/ds1000_pset_0.pdf) and in this [gif tutorial](https://github.com/jbisbee1/DS1000_F2024/blob/main/Psets/save_as_pdf.gif). 

This problem set is worth 5 total points, plus 2 extra credit points. The point values for each question are indicated in brackets below. To receive full credit, you must have the correct code. In addition, some questions ask you to provide a written response in addition to the code.

You will be deducted 1 point for each day late the problem set is submitted, and 1 point for failing to submit in the correct format.

You are free to rely on whatever resources you need to complete this problem set, including lecture notes, lecture presentations, Google, your classmates...you name it. However, the final submission must be complete by you. There are no group assignments.

*Note that the TAs and professors will not respond to Campuswire posts after 5PM on Friday, so don't wait until the last minute to get started!*

**Good luck!**

*Copy the link to ChatGPT you used here: _________________


## Question 0
Require `tidyverse` and `tidymodels`, and then load the [`pres_elec.rds`](https://github.com/jbisbee1/DS1000_F2024/raw/main/data/pres_elec.rds) data to an object called `dat`.
```{r,warning=F,echo=F,message=F}
# INSERT CODE HERE
```


## Question 1 [1 point]
*Describe the data. What is the unit of analysis? What information do the columns provide? What is the period described (i.e., how far back in time does the data go?). Is there any missing data? If so, "where" is it, in terms of both columns and in terms of the observations that have missing data?*

```{r}
# INSERT CODE HERE
```

> Write answer here

## Question 2 [1 point]
*Perform k-means analysis on the Republican and Democrat votes and then use these clusters to predict who wins the election. First, generate an elbow plot with k ranging from 2 to 30. Which value of k appears to be the best? [0.2 points]*

```{r,warning=F}
# Looking at multiple values of k
# Instantiate empty object
for() { # Loop over values of k
  # Calculate k-means cluster solution for given value of k
  
  # Save result including value of k and the total WSS
}

# Plotting the elbow plot.
```

> Write answer here.

*Then predict the `GOP_win` binary outcome as a function of the cluster assignment using a logit regression. Make sure to `factor(cluster)` in the regression. What is the AUC for this model? To calculate, use cross validation with an 80-20% split to re-calculate the AUC. Overall, would you say that the k-means algorithm helps you predict which counties vote Republican? [0.8 points]*

```{r}
set.seed(123)
# Calculate final k-means object

# Plotting the result

# Create dataset for analysis

# Estimate logit model

# Calculate AUC

# Calculate cross-validated result

# Summarise the overall average AUC across cross-validated runs
```

> Write answer here

## Question 3 [1 point]
*Re-do the preceding analysis except instead of using total votes, calculate the percent vote share for Democrats and Republicans in each county (i.e., the two-party vote share) [0.1 points]. Then identify the optimal value of k using the elbow plot visualization, use this as your value of k for the clustering solution and again plot the results [0.2 points]. Again use a logit regression to predict `GOP_win` as a function of the cluster membership for each county and calculate the AUC using the same cross validation method [0.3 points]. Does your answer change? Why? [0.4 points]*

```{r,warning=F}
# Wrangle data to get DEM and REP vote shares

# Re-calculate elbow plot

# Plotting the elbow plot

# Rerunning with optimal k based on elbow plot

# Plot the result

# Create the dataset for analysis

# Estimate logit model

# Calculate AUC

# Calculate cross-validated AUC

# Calculate overall average AUC from cross-validation
```

> Write answer here


## Question 4 [1 point]
*The final two questions will use a different dataset: Donald Trump's tweets. Require `tidyverse`, `tidytext` and `tidymodels`, and then load the [`Trump_tweet_words.Rds`](https://github.com/jbisbee1/DS1000_F2024/raw/main/data/Trump_tweet_words.Rds) data to an object called `tweet_words`.*
```{r,warning=F,message=F}
# INSERT CODE HERE
```

*Plot the total number of times the word "trump" is used each year. When did he appear to talk about himself the most? [0.25 points]*

```{r}
# INSERT CODE HERE
```

> Write answer here

*Then, plot the proportion of times the word "trump" is used each year. According to this plot, when did he appear to talk about himself the most? [0.25 points]*

```{r,warning = F}
# INSERT CODE HERE
```

> Write answer here

*Why are these plots so different? Which measure is better? Why? [0.5 points]*

> Write answer here

## Question 5 [1 point]

*We want to only look at tweets written during 2016 when Trump was campaigning to become president, and are interested if there are patterns in what he talks about.*

*We will use $k$-means clustering to learn about this data. To do so, follow these steps.*

*a. Create a document-term matrix (`dtm`), dropping any words that appear fewer than 20 times total, and using the `document` column as the document indicator.* **NB: Drop the word `'amp'`.** *[0.2 points]*

*b. Calculate the TF-IDF using the appropriate function from the `tidytext` package. [0.1 points]*

*c. Cast the DTM to wide format using the `cast_dtm()` function, also from the `tidytext` package. [0.1 points]*

*d. Determine the optimal number of clusters / centers / topics / $k$ by creating and manually inspecting an elbow plot. To save time, only examine the following sizes: `c(1,10,50,100,250,500,1000)` and set `nstart = 5` with `set.seed(123)`. (This will still take a little while to run so be patient!). [0.3 points]*

*e. Using the optimal value from the elbow plot, run $k$-means on the data with `nstart` set to 5 and `set.seed(123)`. [0.1 points]*

*f. Which are the top 3 most popular topics for Donald Trump in this period? Plot the top 10 highest scoring words for each of the top 3 most popular topics. What is each "about"? [0.2 points]*

```{r}
# a.
dtm <- tweet_words %>% 
  filter(..., # Filter to the correct period
         ...) %>%  # Drop the word 'amp'
  count(...) %>%  # Count the number of times each word appears in each document
  group_by(...) %>%  # Count the total number of times a word appears overall
  mutate(tot_n = ...) %>% 
  ungroup() %>% 
  filter(...) # Filter to only words that appear more than 20 times in total

#b.
dtm.tfidf <- bind_tf_idf(tbl = ..., 
                         term = ..., 
                         document = ..., 
                         n = ...) # Calculate the TF-IDF metric

#c.
castdtm <- cast_dtm(data = ..., 
                    document = ..., 
                    term = ..., 
                    value = ...) # Cast to a DTM

#d. 
set.seed(123) # Set common seed to ensure reproducability
# Instantiate an empty object
for(k in ...) { # Loop over possible values of k
  # Run the k-means function
  km_out <- kmeans(..., 
                   centers = ..., # Set the number of centers equal to k
                   nstart = ...) # Set nstart = 5
  
  # Save the results
  totWSS <- data.frame(totWSS = ...,
                       k = ...) %>%
    bind_rows(...)
  cat(k,'\n')
}

# Plot the elbow plot

#e. 
km_out <- kmeans(..., 
                 centers = ..., # Set the number of centers to the value identified in the elbow plot
                 nstart = ...)  # Set nstart = 5

km_out_tidy <- tidy(...) %>%   # Tidy the kmeans result
  pivot_longer(names_to = ..., # Set the name of the new column for the words
               values_to = ...,# Set the name of the new column for the average TF-IDF values
               cols = c(-...)) # Keep every column EXCEPT `size`, `cluster`, and `withinss`

#f. Find the top 3 topics tweeted by Trump
(tops <- km_out_tidy %>% 
  arrange(desc(...)) %>% # Arrange in descending order of how many documents were about each topic
    slice(...)) # Slice the top 3 topics

#g. Visualize the top 10 words for these top 3 topics
km_out_tidy %>%
  filter(cluster %in% ...) %>% # Grab the top 3 topics from above
  group_by(...) %>% # Filter to the top 10 highest scoring words
  arrange(-...) %>%
  slice(...) %>%
  ggplot(aes(x = ..., # Put the average TF-IDF on the x-axis
             y = reorder_within(...), # Reorder the top 10 words by the x-axis, within clusters
             fill = factor(...))) + # Fill the bars by the topic
  geom_...(...) + # Choose the best geom
  scale_y_reordered() + # Add this to keep the scales looking nice
  facet_wrap(~...) + # Create facets 
  labs(...) # Add labels
```

> Write answer here

## Extra Credit [2 points]
*Load the sentiment dictionary `nrc` from the `tidytext` package, and look at the clusters with sentiment scores by merging the `km_out_tidy` dataset with the `nrc` dataset using the `inner_join()` function. (If you can't open the `nrc` object from the `tidytext` package, you can just load it from GitHub with this link: `https://github.com/jbisbee1/DS1000_F2024/raw/main/data/nrc.Rds`). Filter to only look at `positive` and `negative` categories and then `select()` only the `size`, `cluster`, `word`, `mean_tfidf`, and `sentiment` columns. Then use `pivot_wider()` to create two columns of `mean_tfidf` values: one for `positive` and one for `negative`. Replace `NA` values with 0! Finally, filter to only look at clusters with more than 20 tweets in them. Save this processed data to an object named `cluster_sentiment`. [1 point] Using this data, plot the top 10 words for the three most negative clusters. Describe what you see. What are Trump’s most negative topics about? [1 point]*

```{r,warning=F}
nrc <- read_rds('https://github.com/jbisbee1/DS1000_F2024/raw/main/data/nrc.Rds') # Load the NRC dictionary

# INSERT CODE HERE
```

> Write answer here

