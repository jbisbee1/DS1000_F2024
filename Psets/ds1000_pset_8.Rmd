---
title: "Problem Set 8"
author: "[YOUR NAME]"
date: "Due Date: 2024-11-08"
output:
  html_document: default
  pdf_document: default
subtitle: Classification
institute: Vanderbilt University
---

```{r,include=F}
knitr::opts_chunk$set(error=TRUE)
```


## Getting Set Up

Open `RStudio` and create a new RMarkDown file (`.Rmd`) by going to `File -> New File -> R Markdown...`.
Accept defaults and save this file as `[YOUR NAME]_ps8.Rmd` to your `code` folder.

Copy and paste the contents of this `.Rmd` file into your `[YOUR NAME]_ps8.Rmd` file. Then change the `author: [Your Name]` on line 2 to your name.

We will be using the fn_cleaned_final.Rds file from the course [github page](https://github.com/jbisbee1/DS1000_F2024/blob/main/data/fn_cleaned_final.Rds). The codebook for this dataset is produced below.

| Name           | Description                                                                                                                                                                                             |
|----------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| player         | A unique ID for each player        |
| gameId   | A unique ID for each game    |
| startTime   | When the game started       |
| sessionId        | A unique ID for each gaming session            |
| gameIdSession       | A unique ID for each game within a session               |
| won             | A binary variable indicating whether the game was won or not |
| mental_state   | Whether the player was drunk or sober when they played                                    |
| eliminations   | How many times they killed an enemy player                                                |
| assists        | How many times they helped a teammate kill an enemy player                                |
| revives | How many times was the player revived (brought back to life)? |
| accuracy       | The proportion of total shots that hit an enemy player                                    |
| hits           | The number of shots that hit an enemy player                                              |
| head_shots     | The number of shots that hit an enemy player in the head                                  |
| distance_traveled | How far did the player move in the game? |
| materials_gathered | How many materials did the player gather in the game? |
| materials_used | How many materials did the player use in the game? |
| damage_taken   | The amount of damage the player received from enemy players                               |
| damage_to_players | How much damage did the player commit to other players in the game? | 
| damage_to_structures | How much damage did the player commit to structures in the game? |


All of the following questions should be answered in this `.Rmd` file. There are code chunks with incomplete code that need to be filled in. 

All of the following questions should be answered in this `.Rmd` file. There are code chunks with incomplete code that need to be filled in. 

This problem set is worth 5 total points, plus **one** extra credit question. However, there are also additional opportunities for additional points in problem 5 and the extra credit question itself. Note that these additional points are **very hard**. They are designed for the students who claimed that the course is easy on the midterm evals. I encourage you all to attempt all extra credit, but don't worry if you don't get the super hard ones. 

The point values for each question are indicated in brackets below. To receive full credit, you must have the correct code. In addition, some questions ask you to provide a written response in addition to the code.
All of the following questions should be answered in this `.Rmd` file. There are code chunks with incomplete code that need to be filled in. To submit, compile (i.e., `knit`) the completed problem set and upload the PDF file to Brightspace on Friday by midnight. Instructions for how to compile the output as a PDF can be found in [Problem Set 0](https://github.com/jbisbee1/DS1000_F2024/blob/main/Psets/ds1000_pset_0.pdf) and in this [gif tutorial](https://github.com/jbisbee1/DS1000_F2024/blob/main/Psets/save_as_pdf.gif). 

This problem set is worth 5 total points, plus 1 extra credit point. The point values for each question are indicated in brackets below. To receive full credit, you must have the correct code. In addition, some questions ask you to provide a written response in addition to the code.

You will be deducted 1 point for each day late the problem set is submitted, and 1 point for failing to submit in the correct format.

You are free to rely on whatever resources you need to complete this problem set, including lecture notes, lecture presentations, Google, your classmates...you name it. However, the final submission must be complete by you. There are no group assignments.

*Note that the TAs and professors will not respond to Campuswire posts after 5PM on Friday, so don't wait until the last minute to get started!*

**Good luck!**

*Copy the link to ChatGPT you used here: _________________


## Question 0
*Require `tidyverse`, `tidymodels` and `ranger`, and then load the [`fn_cleaned_final.Rds`](https://github.com/jbisbee1/DS1000_F2024/raw/main/data/fn_cleaned_final.rd') data to an object called `fn`.*
```{r,message=F}
# INSERT CODE HERE
```


## Question 1 [1 point]
*In this problem set, we are interested in developing a classifier that maximizes our accuracy for predicting Fortnite victories. To do so we will use a linear probability model, a logit, and a random forest. We will start by using two $X$ variables to predict the probability of winning: accuracy (`accuracy`), and head shots (`head_shots`). Our outcome variable of interest $Y$ is whether the player won the game (`won`).*

*Start by* **looking** *at these variables. Why types of variables are they? How much missingness do they have? What do their univariate visualizations look like?*

```{r,message=F}
# INSERT CODE HERE
```

> Write answer here

*Then create two multivariate visualizations of the relationship between `won` and each of the two $X$ variables one-by-one.* 
```{r,message=F}
# INSERT CODE HERE
```

*Finally, use `geom_tile()` to create a heatmap of the three-way relationship, where quintiles of `accuracy` is on the x-axis, quintiles of `head_shots` is on the y-axis, and tiles are filled according to the average winning probability. (NB: look up what "quintile" means if you are not sure.) Is there anything surprising about this result?*
```{r,message=F}
# Multivariate: 3-dimensions
fn %>%
  mutate(accuracy_quintile = ntile(...), # Calculate quintiles for accuracy and headshots
         head_quintile = ntile(...)) %>%
  group_by() %>% # Calculate probability of winning by accuracy and headshot quintiles
  summarise(prob_win = ...) %>%
  ggplot(aes(x = ...,  # Create heatmap with accuracy quintiles on the x-axis
             y = ...,  # headshot quintiles on the y-axis
             fill = ...)) +  # And fill by the probability of winning
  geom_...() +  # Use the correct geom
  labs(...) # Labels for the win!
```

> Write answer here

## Question 2 [1 point]
*Now let's run a linear model and evaluate it in terms of overall accuracy, sensitivity and specificity using a threshold of 0.5.*

```{r,message=F}
# Running linear model
model_lm <- lm(formula = ..., # Define the regression equation
               data = ...) # Provide the dataset

# Calculating accuracy, sensitivity, and specificity
fn %>%
  mutate(prob_win = ...) %>% # Calculate the probability of winning
  mutate(pred_win = ...) %>% # Convert the probability to a 1 if the probability is greater than 0.5, or zero otherwise
  group_by(...) %>% # Calculate the total games by whether they were actually won or lost
  mutate(total_games = ...) %>%
  group_by(...) %>% # Calculate the number of games by whether they were actually won or lost, and by whether they were predicted to be won or lost
  summarise(nGames=...) %>%
  mutate(prop = ...) %>% # Calculate the proportion of game by the total games
  ungroup() %>%
  mutate(accuracy = ...) # Calculate the overall accuracy
```

*Then, determine the threshold that maximizes both specificity and sensitivity by creating a sentivity-vs-specificity plot and adding a vertical line to pinpoint where the two measures intersect.*
```{r,message=F}
# Create the sensitivity vs specificity plot
toplot <- NULL # Instantiate an empty object
for(thresh in ...) { # sequence from 0 to 1 by 0.025
  toplot <- fn %>%
  mutate(prob_win = ...) %>% # Calculate the probability of winning
  mutate(pred_win = ...) %>% # Convert the probability to a 1 if the probability is greater than the given threshold, or zero otherwise
  group_by(...) %>% # Calculate the total games by whether they were actually won or lost
  mutate(total_games = ...) %>%
  group_by(...) %>% # Calculate the number of games by whether they were actually won or lost, and by whether they were predicted to be won or lost
  summarise(nGames=...) %>%
  mutate(prop = ...) %>% # Calculate the proportion of game by the total games
  ungroup() %>%
  mutate(accuracy = ...) %>% # Calculate the overall accuracy
  mutate(threshold = ...) %>% # Record the threshold level
    bind_rows(...) # Add it to the toplot object
}

toplot %>%
  mutate(metric = ifelse(...)) %>% # Using an ifelse() function, label each row as either Sensitivity (if the predicted win is 1 and the true win is 1), Specificity (if the predicted win is 0 and the true win is 0), or NA
  drop_na(...) %>% # Drop rows that are neither sensitivity nor specificity measures
  ggplot(aes(x = ...,
             y = ...,
             color = ...)) + # Visualize the Sensitivity and Specificity curves by putting the threshold on the x-axis, the proportion of all games on the y-axis, and coloring by Sensitivity or Specificity
  geom_...() + 
  geom_vline(xintercept = ...) # Find the best threshold!
```

> Write answer here

*Finally, plot the area under the curve (AUC) and then calculate it. What grade would you give this model?*
```{r,message=F}
# Plot the AUC
toplot %>%
  mutate(metric = ifelse(...)) %>% # Using an ifelse() function, label each row as either Sensitivity (if the predicted win is 1 and the true win is 1), Specificity (if the predicted win is 0 and the true win is 0), or NA
  drop_na(...) %>% # Drop rows that are neither sensitivity nor specificity measures
  select(...) %>% # Select only the prop, metric, and threshold columns
  pivot_wider(names_from = ...,
              values_from = ...) %>% # Pivot the data to wide format, where the new columns should be the metric
  arrange(desc(...),...) %>% # Arrange by descending specificity, and then by sensitivity
  ggplot(aes(x = 1-..., # Plot 1 minus the Specificity on the x-axis
             y = ...)) +  # Plot the Sensitivity on the y-axis
  geom_line() + 
  xlim(...) + ylim(...) + # Expand the x and y-axis limits to be between 0 and 1
  geom_abline(...) + # Add a 45-degree line using geom_abline()
  labs(...) # Add clear labels! (Make sure to indicate that this is the result of a linear regression model)

# Calculate the AUC
forAUC <- fn %>%
  mutate(prob_win = ..., # Generate predicted probabilities of winning from our model
         truth = factor(...)) %>% # Conver the outcome to a factor with levels c('1','0')
  select(...) # Select only the probability and true outcome columns

roc_auc(data = ..., # Run the roc_auc() function on the dataset we just created
        ..., # Tell it which column contains the true outcomes
        ...) # Tell it which column contains our model's predicted probabilities
```

> Write answer here


## Question 3 [1 point]
*Now let's re-do the exact same work, except use a* **logit model** *instead of a linear model. Based on your analysis, which model has a larger AUC?*

```{r,message=F}
# Running logit model
model_glm <- glm(formula = ..., # Define the regression equation
               data = ..., # Provide the dataset
               family = ...) # Specify the link function

# Calculating accuracy, sensitivity, and specificity
fn %>%
  mutate(prob_win = predict(...)) %>% # Calculate the probability of winning (don't forget to set type = 'response' for the glm()!)
  mutate(pred_win = ...) %>% # Convert the probability to a 1 if the probability is greater than 0.5, or zero otherwise
  group_by(...) %>% # Calculate the total games by whether they were actually won or lost
  mutate(total_games = ...) %>%
  group_by(...) %>% # Calculate the number of games by whether they were actually won or lost, and by whether they were predicted to be won or lost
  summarise(nGames=...) %>%
  mutate(prop = ...) %>% # Calculate the proportion of game by the total games
  ungroup() %>%
  mutate(accuracy = ...) # Calculate the overall accuracy

# Create the sensitivity vs specificity plot
toplot <- NULL # Instantiate an empty object
for(thresh in ...) { # sequence from 0 to 1 by 0.025
  toplot <- fn %>%
  mutate(prob_win = predict(...)) %>% # Calculate the probability of winning (don't forget to set type = 'response' for the glm()!)
  mutate(pred_win = ifelse(...)) %>% # Convert the probability to a 1 if the probability is greater than the given threshold, or zero otherwise
  group_by(...) %>% # Calculate the total games by whether they were actually won or lost
  mutate(total_games = ...) %>%
  group_by(...) %>% # Calculate the number of games by whether they were actually won or lost, and by whether they were predicted to be won or lost
  summarise(nGames=...) %>%
  mutate(prop = ...) %>% # Calculate the proportion of game by the total games
  ungroup() %>%
  mutate(accuracy = ...) %>% # Calculate the overall accuracy
  mutate(threshold = ...) %>% # Record the threshold level
    bind_rows(...) # Add it to the toplot object
}

toplot %>%
  mutate(metric = ifelse(...)) %>% # Using an ifelse() function, label each row as either Sensitivity (if the predicted win is 1 and the true win is 1), Specificity (if the predicted win is 0 and the true win is 0), or NA
  drop_na(...) %>% # Drop rows that are neither sensitivity nor specificity measures
  ggplot(aes(x = ...,
             y = ...,
             color = ...)) + # Visualize the Sensitivity and Specificity curves by putting the threshold on the x-axis, the proportion of all games on the y-axis, and coloring by Sensitivity or Specificity
  geom_...() + 
  geom_vline(xintercept = ...)

# Plot the AUC
toplot %>%
  mutate(metric = ifelse(...)) %>% # Using an ifelse() function, label each row as either Sensitivity (if the predicted win is 1 and the true win is 1), Specificity (if the predicted win is 0 and the true win is 0), or NA
  drop_na(...) %>% # Drop rows that are neither sensitivity nor specificity measures
  select(...) %>% # Select only the prop, metric, and threshold columns
  pivot_wider(names_from = ...,
              values_from = ...) %>% # Pivot the data to wide format, where the new columns should be the metric
  arrange(desc(...),...) %>% # Arrange by descending specificity, and then by sensitivity
  ggplot(aes(x = 1-..., # Plot 1 minus the Specificity on the x-axis
             y = ...)) +  # Plot the Sensitivity on the y-axis
  geom_...() + 
  xlim(...) + ylim(...) + # Expand the x and y-axis limits to be between 0 and 1
  geom_abline(...) + # Add a 45-degree line using geom_abline()
  labs(...)# Add clear labels! (Make sure to indicate that this is the result of a logit regression model)

# Calculate the AUC
forAUC <- fn %>%
  mutate(prob_win = predict(...), # Generate predicted probabilities of winning from our model
         truth = factor(...)) %>% # Conver the outcome to a factor with levels c('1','0')
  select(...) # Select only the probability and true outcome columns

roc_auc(data = ..., # Run the roc_auc() function on the dataset we just created
        ..., # Tell it which column contains the true outcomes
        ...) # Tell it which column contains our model's predicted probabilities
```

> Write answer here


## Question 4 [1 points]
*Now let's re-do the exact same work, except use a* **random forest** *model instead of a linear model. Based on your analysis, which model has a larger AUC?*

```{r,message=F}
# Running random forest model
model_rf <- ranger(formula = ..., # Define the regression equation
               data = ...) # Specify the link function

# Calculating accuracy, sensitivity, and specificity
fn %>%
  mutate(prob_win = predict(...)$predictions) %>% # Calculate the probability of winning (don't forget to use the $ sign to get the predictions for random forests!)
  mutate(pred_win = ifelse(...)) %>% # Convert the probability to a 1 if the probability is greater than 0.5, or zero otherwise
  group_by(...) %>% # Calculate the total games by whether they were actually won or lost
  mutate(total_games = ...) %>%
  group_by(...) %>% # Calculate the number of games by whether they were actually won or lost, and by whether they were predicted to be won or lost
  summarise(nGames=...) %>%
  mutate(prop = ...) %>% # Calculate the proportion of game by the total games
  ungroup() %>%
  mutate(accuracy = ...) # Calculate the overall accuracy

# Create the sensitivity vs specificity plot
toplot <- NULL # Instantiate an empty object
for(thresh in ...) { # sequence from 0 to 1 by 0.025
  toplot <- fn %>%
  mutate(prob_win = predict(...)$predictions) %>% # Calculate the probability of winning (don't forget to use the $ sign to get the predictions for random forests!)
  mutate(pred_win = ifelse(...)) %>% # Convert the probability to a 1 if the probability is greater than 0.5, or zero otherwise
  group_by(...) %>% # Calculate the total games by whether they were actually won or lost
  mutate(total_games = ...) %>%
  group_by(...) %>% # Calculate the number of games by whether they were actually won or lost, and by whether they were predicted to be won or lost
  summarise(nGames=...) %>%
  mutate(prop = ...) %>% # Calculate the proportion of game by the total games
  ungroup() %>%
  mutate(accuracy = ...) %>% # Calculate the overall accuracy
  mutate(threshold = ...) %>% # Record the threshold level
    bind_rows(...) # Add it to the toplot object
}

toplot %>%
  mutate(metric = ifelse(...)) %>% # Using an ifelse() function, label each row as either Sensitivity (if the predicted win is 1 and the true win is 1), Specificity (if the predicted win is 0 and the true win is 0), or NA
  drop_na(...) %>% # Drop rows that are neither sensitivity nor specificity measures
  ggplot(aes(x = ...,
             y = ...,
             color = ...)) + # Visualize the Sensitivity and Specificity curves by putting the threshold on the x-axis, the proportion of all games on the y-axis, and coloring by Sensitivity or Specificity
  geom_...() + 
  geom_vline(xintercept = ...)

# Plot the AUC
toplot %>%
  mutate(metric = ifelse(...)) %>% # Using an ifelse() function, label each row as either Sensitivity (if the predicted win is 1 and the true win is 1), Specificity (if the predicted win is 0 and the true win is 0), or NA
  drop_na(...) %>% # Drop rows that are neither sensitivity nor specificity measures
  select(...) %>% # Select only the prop, metric, and threshold columns
  pivot_wider(names_from = ...,
              values_from = ...) %>% # Pivot the data to wide format, where the new columns should be the metric
  arrange(desc(...),...) %>% # Arrange by descending specificity, and then by sensitivity
  ggplot(aes(x = 1-..., # Plot 1 minus the Specificity on the x-axis
             y = ...)) +  # Plot the Sensitivity on the y-axis
  geom_...() + 
  xlim(...) + ylim(...) + # Expand the x and y-axis limits to be between 0 and 1
  geom_abline(...) + # Add a 45-degree line using geom_abline()
  labs(...) # Add clear labels! (Make sure to indicate that this is the result of a logit regression model)

# Calculate the AUC
forAUC <- fn %>%
  mutate(prob_win = predict(...)$predictions, # Generate predicted probabilities of winning from our model (don't forget to use the $ sign to get the predictions for random forests!)
         truth = factor(...)) %>% # Convert the outcome to a factor with levels c('1','0')
  select(...) # Select only the probability and true outcome columns

roc_auc(data = ..., # Run the roc_auc() function on the dataset we just created
        ..., # Tell it which column contains the true outcomes
        ...) # Tell it which column contains our model's predicted probabilities
```

> Write answer here

## Question 5 [1 point]
*Use 100-fold cross validation with a 60-40 split to calculate the average AUC for all three models. Which is better?*


```{r,message=F}
set.seed(123)
cvRes <- NULL
for(...) {
  # Cross validation prep
  train <- fn %>% 
    sample_n(...)
  
  test <- fn %>% 
    anti_join(...)

  # Training models
  mLM <- lm(formula =...,
            data = ...)
  mGLM <- glm(formula = ...,
            data = ...,
            family = ...)
  mRF <- ranger(formula = ...,
                data = ...)
  
  # Predicting models
  toEval <- test %>%
    mutate(mLMPreds = predict(...),
           mGLMPreds = predict(...),
           mRFPreds = predict(...)$predictions,
           truth = factor(...))

  # Evaluating models
  rocLM <- roc_auc(...) %>%
    mutate(model = 'linear') %>%
    rename(auc = .estimate)
    
  rocGLM <- roc_auc(...) %>%
    mutate(model = 'logit') %>%
    rename(auc = .estimate)
  
  rocRF <- roc_auc(...) %>%
    mutate(model = 'rf') %>%
    rename(auc = .estimate)

  cvRes <- rocLM %>%
    bind_rows(rocGLM,
              rocRF) %>%
    mutate(cvInd = i) %>%
    bind_rows(cvRes)
}

cvRes %>%
  group_by(...) %>%
  summarise(mean_auc = ...)
```

> Write answer here

## Extra Credit [2 points]
*Now let's re-do EVERYTHING from the previous questions using a "kitchen sink" model with many different predictors. We will use the following $X$ variables:*

- `hits`
- `assists`
- `accuracy`
- `head_shots`
- `damage_to_players`
- `eliminations`
- `revives`
- `distance_traveled`
- `materials_gathered`
- `mental_state`

*Which of these variables do you think will be* **most important** *for predicting whether the player wins the game? Why? What about the variables that are* **least important***? Why?*

> Write answer here

*Now run a random forest that uses all of these variables as predictors and use `importance = 'permutation'` to see which variables the random forest thinks are most important. Visualize these results with a barplot. Where do the variables you thought would be best and worst appear?*

```{r,message=F}
# INSERT CODE HERE
```

> Write answer here


*What is the overall AUC from your random forest model using all these variables?*

```{r,message=F}
# INSERT CODE HERE
```

> Write answer here

*Then use 100 cross validation with a 60-40 split to get a better measure of the true model performance. Is the AUC from running the model on the full data different from the cross validation answer? Calculate the proportion of cross validation splits where the AUC is worse than the value calculated on the full data. Do you think there is evidence of overfitting?*

```{r,message=F}
# INSERT CODE HERE
```

> Write answer here

